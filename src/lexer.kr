// ============================================================================
// KORE Lexer v2 - Efficient Implementation
// ============================================================================
// Key improvements over v1:
// - NO split(source, "") - uses char_at() on original source
// - Integer comparisons via ord() instead of str_eq()
// - Tracks paren_depth for implicit line joining
// - Token stores start/end indices for lazy string allocation
// ============================================================================

use span

// =============================================================================
// Token Types
// =============================================================================

enum TokenType:
    // Literals
    Int(Int)
    Float(Float)
    String(String)
    Bool(Bool)
    None
    
    // Identifiers and Keywords
    Ident(String)
    Keyword(String)
    
    // Operators
    Plus
    Minus
    Star
    Slash
    Percent
    Eq
    EqEq
    NotEq
    Lt
    LtEq
    Gt
    GtEq
    And
    Or
    Not
    Arrow      // ->
    FatArrow   // =>
    Dot
    DotDot     // ..
    Colon
    ColonColon // ::
    Comma
    Semicolon
    Pipe       // |
    Ampersand  // &
    
    // Brackets
    LParen
    RParen
    LBracket
    RBracket
    LBrace
    RBrace
    
    // Whitespace (significant in KORE!)
    Newline
    Indent
    Dedent
    
    // Special
    Eof
    Error(String)

// =============================================================================
// Token
// =============================================================================

struct Token:
    type: TokenType
    span: Span
    lexeme: String

impl Token:
    pub fn new(type: TokenType, span: Span, lexeme: String) -> Token:
        return Token { type: type, span: span, lexeme: lexeme }
    
    pub fn is_keyword(self, kw: String) -> Bool:
        if !str_eq(variant_of(self.type), "Keyword"):
            return false
        return str_eq(self.lexeme, kw)
    
    pub fn is_ident(self) -> Bool:
        return str_eq(variant_of(self.type), "Ident")

    pub fn is_error(self) -> Bool:
        return str_eq(variant_of(self.type), "Error")
    
    pub fn is_indent(self) -> Bool:
        return str_eq(variant_of(self.type), "Indent")
    
    pub fn is_dedent(self) -> Bool:
        return str_eq(variant_of(self.type), "Dedent")
    
    pub fn is_newline(self) -> Bool:
        return str_eq(variant_of(self.type), "Newline")

    pub fn is_eof(self) -> Bool:
        return str_eq(variant_of(self.type), "Eof")

// =============================================================================
// Lexer v2 - Efficient, no char array splitting
// =============================================================================

struct Lexer:
    source: String       // Original source - we index into this directly
    source_len: Int      // Cached length to avoid repeated strlen calls
    pos: Int             // Current position in source
    line: Int
    column: Int
    indent_stack: Array<Int>
    paren_depth: Int     // Track ()[]{}  for implicit line joining
    file: String         // Source filename

impl Lexer:
    pub fn new(source: String, file: String) -> Lexer:
        let len = str_len(source)
        let stack = []
        push(stack, 0)
        return Lexer {
            source: source,
            source_len: len,
            pos: 0,
            line: 1,
            column: 1,
            indent_stack: stack,
            paren_depth: 0,
            file: file
        }

// =============================================================================
// Character access helpers - NO ARRAY, just char_at
// =============================================================================

fn lexer_is_eof(lexer: Lexer) -> Bool:
    return lexer.pos >= lexer.source_len

fn lexer_peek(lexer: Lexer) -> Int:
    // Returns character code (Int) - NOT a string!
    if lexer_is_eof(lexer):
        return 0
    return char_code_at(lexer.source, lexer.pos)

fn lexer_peek_n(lexer: Lexer, n: Int) -> Int:
    let idx = lexer.pos + n
    if idx >= lexer.source_len:
        return 0
    return char_code_at(lexer.source, idx)

fn lexer_advance(lexer: Lexer) -> Int:
    if lexer_is_eof(lexer):
        return 0
    let code = char_code_at(lexer.source, lexer.pos)
    println("DEBUG: lexer=" + to_string(lexer) + " advance from " + to_string(lexer.pos) + " char=" + to_string(code))
    lexer.pos = lexer.pos + 1
    if code == 10:  // '\n'
        lexer.line = lexer.line + 1
        lexer.column = 1
    else:
        lexer.column = lexer.column + 1
    return code

fn lexer_make_token(lexer: Lexer, type: TokenType, start: Int, end: Int) -> Token:
    let lexeme = substring(lexer.source, start, end)
    let span = Span::new(start, end, lexer.file)
    return Token::new(type, span, lexeme)

fn lexer_make_simple_token(lexer: Lexer, type: TokenType, lexeme: String) -> Token:
    let end = lexer.pos
    let start = end - str_len(lexeme)
    if start < 0:
        start = 0
    let span = Span::new(start, end, lexer.file)
    return Token::new(type, span, lexeme)

// =============================================================================
// Character classification helpers - ALL use integer codes
// =============================================================================

fn is_digit(code: Int) -> Bool:
    return code >= 48 && code <= 57  // '0'-'9'

fn is_alpha(code: Int) -> Bool:
    let lower = code >= 97 && code <= 122   // 'a'-'z'
    let upper = code >= 65 && code <= 90    // 'A'-'Z'
    return lower || upper

fn is_ident_start(code: Int) -> Bool:
    return is_alpha(code) || code == 95  // '_'

fn is_ident_char(code: Int) -> Bool:
    return is_alpha(code) || is_digit(code) || code == 95

fn is_whitespace(code: Int) -> Bool:
    return code == 32 || code == 9 || code == 13  // ' ', '\t', '\r'

// =============================================================================
// Keyword lookup - NO array allocation per call
// =============================================================================

fn is_keyword(s: String) -> Bool:
    if str_eq(s, "fn"): return true
    if str_eq(s, "let"): return true
    if str_eq(s, "var"): return true
    if str_eq(s, "if"): return true
    if str_eq(s, "else"): return true
    if str_eq(s, "while"): return true
    if str_eq(s, "for"): return true
    if str_eq(s, "in"): return true
    if str_eq(s, "return"): return true
    if str_eq(s, "match"): return true
    if str_eq(s, "struct"): return true
    if str_eq(s, "enum"): return true
    if str_eq(s, "impl"): return true
    if str_eq(s, "use"): return true
    if str_eq(s, "pub"): return true
    if str_eq(s, "async"): return true
    if str_eq(s, "await"): return true
    if str_eq(s, "spawn"): return true
    if str_eq(s, "actor"): return true
    if str_eq(s, "on"): return true
    if str_eq(s, "send"): return true
    if str_eq(s, "true"): return true
    if str_eq(s, "false"): return true
    if str_eq(s, "none"): return true
    if str_eq(s, "self"): return true
    if str_eq(s, "test"): return true
    if str_eq(s, "comptime"): return true
    if str_eq(s, "with"): return true
    if str_eq(s, "break"): return true
    if str_eq(s, "continue"): return true
    if str_eq(s, "loop"): return true
    if str_eq(s, "trait"): return true
    if str_eq(s, "const"): return true
    if str_eq(s, "extern"): return true
    return false

// =============================================================================
// Main tokenizer - Efficient implementation
// =============================================================================

pub fn tokenize(lexer: Lexer) -> Array<Token>:
    println("DEBUG: tokenize start")
    let tokens = []
    let at_line_start = true
    var loop_count = 0
    
    loop:
        if lexer_is_eof(lexer):
            break
            
        loop_count = loop_count + 1
        // The user's provided snippet had a malformed println and a duplicate lexer_peek.
        // Assuming the intent was to replace the `if loop_count < 100` block with a single debug print.
        // And to ensure `let c = lexer_peek(lexer)` is only called once per loop iteration.
        if loop_count < 100:
            println("DEBUG: tokenize loop " + to_string(loop_count) + " pos=" + to_string(lexer.pos))
        
        if loop_count % 1000 == 0:
             println("DEBUG: tokenize loop " + to_string(loop_count) + " pos=" + to_string(lexer.pos))
        
        let c = lexer_peek(lexer)
        
        // === Handle line start - indentation ===
        if at_line_start:
            // Skip blank lines and CRLF
            if c == 10:  // '\n'
                let _ = lexer_advance(lexer)
                // Only emit Newline if not inside parens
                if lexer.paren_depth == 0:
                    push(tokens, lexer_make_simple_token(lexer, TokenType::Newline, "\n"))
                continue
            
            if c == 13:  // '\r'
                let _ = lexer_advance(lexer)
                continue
            
            // Skip comments on blank lines
            if c == 47 && lexer_peek_n(lexer, 1) == 47:  // '//'
                skip_line_comment(lexer)
                continue
            if c == 35:  // '#'
                skip_line_comment(lexer)
                continue
            
            // Count indentation
            let indent = 0
            loop:
                let ch = lexer_peek(lexer)
                if ch == 32:  // ' '
                    indent = indent + 1
                    let _ = lexer_advance(lexer)
                else if ch == 9:  // '\t'
                    indent = indent + 4
                    let _ = lexer_advance(lexer)
                else:
                    break
            
            // Check what's after whitespace
            let next = lexer_peek(lexer)
            
            // Blank line - skip without changing indent
            if next == 13 || next == 10 || next == 0:
                if next == 13:
                    let _ = lexer_advance(lexer)
                if lexer_peek(lexer) == 10:
                    let _ = lexer_advance(lexer)
                at_line_start = true
                continue
            
            // Comment line - skip
            if next == 47 && lexer_peek_n(lexer, 1) == 47:
                skip_line_comment(lexer)
                at_line_start = true
                continue
            if next == 35:
                skip_line_comment(lexer)
                at_line_start = true
                continue
            
            // Real code - apply indentation logic (only if not inside parens)
            at_line_start = false
            
            if lexer.paren_depth == 0:
                let stack_len = array_len(lexer.indent_stack)
                let last_idx = stack_len - 1
                let current_indent = lexer.indent_stack[last_idx]
                
                if indent > current_indent:
                    push(lexer.indent_stack, indent)
                    push(tokens, lexer_make_simple_token(lexer, TokenType::Indent, ""))
                else if indent < current_indent:
                    // Emit Dedents
                    let dedent_len = array_len(lexer.indent_stack)
                    while dedent_len > 1:
                        let top_idx = dedent_len - 1
                        let top = lexer.indent_stack[top_idx]
                        if top <= indent:
                            break
                        let _ = pop(lexer.indent_stack)
                        push(tokens, lexer_make_simple_token(lexer, TokenType::Dedent, ""))
                        dedent_len = array_len(lexer.indent_stack)
            
            c = lexer_peek(lexer)
        
        // === Skip inline whitespace ===
        if is_whitespace(c):
            let _ = lexer_advance(lexer)
            continue
        
        // === Skip comments ===
        if c == 47 && lexer_peek_n(lexer, 1) == 47:  // '//'
            skip_line_comment(lexer)
            continue
        if c == 35:  // '#'
            skip_line_comment(lexer)
            continue
        
        // === Newlines ===
        if c == 10:  // '\n'
            let _ = lexer_advance(lexer)
            // Only emit if not inside parentheses (implicit line joining)
            if lexer.paren_depth == 0:
                push(tokens, lexer_make_simple_token(lexer, TokenType::Newline, "\n"))
            at_line_start = true
            continue
        
        if c == 13:  // '\r'
            let _ = lexer_advance(lexer)
            continue
        
        // === Comments ===
        if c == 47:  // '/'
             let next = lexer_peek_n(lexer, 1)
             if next == 47: // '//'
                 skip_line_comment(lexer)
                 continue
        
        if is_digit(c):
            println("DEBUG: tokenize loop match digit")
            let tok = lex_number(lexer)
            push(tokens, tok)
            continue
        
        if is_ident_start(c):
            let tok = lex_ident(lexer)
            push(tokens, tok)
            continue
        
        if c == 34 || c == 39:  // '"' or '\''
            let tok = lex_string(lexer, c)
            push(tokens, tok)
            continue
        
        if lex_multi_char_op(lexer, tokens):
            println("DEBUG: tokenize loop match multi-char")
            continue
        
        println("DEBUG: tokenize loop default advance")
        let _ = lexer_advance(lexer)
        
        if c == 40:  // '('
            lexer.paren_depth = lexer.paren_depth + 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::LParen, "("))
        else if c == 41:  // ')'
            if lexer.paren_depth > 0:
                lexer.paren_depth = lexer.paren_depth - 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::RParen, ")"))
        else if c == 91:  // '['
            lexer.paren_depth = lexer.paren_depth + 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::LBracket, "["))
        else if c == 93:  // ']'
            if lexer.paren_depth > 0:
                lexer.paren_depth = lexer.paren_depth - 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::RBracket, "]"))
        else if c == 123:  // '{'
            lexer.paren_depth = lexer.paren_depth + 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::LBrace, "{"))
        else if c == 125:  // '}'
            if lexer.paren_depth > 0:
                lexer.paren_depth = lexer.paren_depth - 1
            push(tokens, lexer_make_simple_token(lexer, TokenType::RBrace, "}"))
        else if c == 43:   // '+'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Plus, "+"))
        else if c == 45:   // '-'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Minus, "-"))
        else if c == 42:   // '*'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Star, "*"))
        else if c == 47:   // '/'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Slash, "/"))
        else if c == 37:   // '%'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Percent, "%"))
        else if c == 61:   // '='
            push(tokens, lexer_make_simple_token(lexer, TokenType::Eq, "="))
        else if c == 33:   // '!'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Not, "!"))
        else if c == 60:   // '<'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Lt, "<"))
        else if c == 62:   // '>'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Gt, ">"))
        else if c == 38:   // '&'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Ampersand, "&"))
        else if c == 124:  // '|'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Pipe, "|"))
        else if c == 94:   // '^'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Caret, "^"))
        else if c == 46:   // '.'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Dot, "."))
        else if c == 58:   // ':'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Colon, ":"))
        else if c == 44:   // ','
            push(tokens, lexer_make_simple_token(lexer, TokenType::Comma, ","))
        else if c == 59:   // ';'
            push(tokens, lexer_make_simple_token(lexer, TokenType::Semicolon, ";"))
        else:
            // Unknown character
            let p_start = lexer.pos - 20
            if p_start < 0: p_start = 0
            let p_end = lexer.pos + 20
            if p_end > lexer.source_len: p_end = lexer.source_len
            let ctx = substring(lexer.source, p_start, p_end)
            println("Lexer Error: Unknown char code " + str(c) + " at line " + str(lexer.line) + " context: [" + ctx + "]")
            let _ = lexer_advance(lexer)
    
    // Emit remaining Dedents
    while array_len(lexer.indent_stack) > 1:
        let _ = pop(lexer.indent_stack)
        push(tokens, lexer_make_simple_token(lexer, TokenType::Dedent, ""))
    
    push(tokens, lexer_make_simple_token(lexer, TokenType::Eof, ""))
    return tokens

// =============================================================================
// Helper functions
// =============================================================================

fn skip_line_comment(lexer: Lexer) -> Unit:
    loop:
        if lexer_is_eof(lexer):
            break
        let c = lexer_peek(lexer)
        if c == 10:  // '\n'
            break
        let _ = lexer_advance(lexer)

fn lex_number(lexer: Lexer) -> Token:
    let start = lexer.pos
    
    loop:
        if lexer_is_eof(lexer):
            break
        let c = lexer_peek(lexer)
        if !is_digit(c):
            break
        let _ = lexer_advance(lexer)
    
    // Check for float
    if lexer_peek(lexer) == 46 && is_digit(lexer_peek_n(lexer, 1)):
        let _ = lexer_advance(lexer)  // consume '.'
        loop:
            if lexer_is_eof(lexer):
                break
            let c = lexer_peek(lexer)
            if !is_digit(c):
                break
            let _ = lexer_advance(lexer)
        let lexeme = substring(lexer.source, start, lexer.pos)
        return lexer_make_token(lexer, TokenType::Float(to_float(lexeme)), start, lexer.pos)
    
    let lexeme = substring(lexer.source, start, lexer.pos)
    let val = to_int(lexeme)
    return lexer_make_token(lexer, TokenType::Int(val), start, lexer.pos)

fn lex_ident(lexer: Lexer) -> Token:
    println("DEBUG lex_ident ENTRY")
    let start = lexer.pos
    
    loop:
        println("DEBUG lex_ident loop iteration")
        if lexer_is_eof(lexer):
            println("DEBUG lex_ident EOF")
            break
        let c = lexer_peek(lexer)
        if !is_ident_char(c):
            println("DEBUG lex_ident break on !is_ident_char")
            break
        println("DEBUG lex_ident calling advance")
        let _ = lexer_advance(lexer)
    
    println("DEBUG lex_ident creating lexeme")
    let lexeme = substring(lexer.source, start, lexer.pos)
    
    // Handle special keyword literals first - use flat structure to avoid else-if chain bug
    if str_eq(lexeme, "true"):
        return lexer_make_token(lexer, TokenType::Bool(true), start, lexer.pos)
    if str_eq(lexeme, "false"):
        return lexer_make_token(lexer, TokenType::Bool(false), start, lexer.pos)
    if str_eq(lexeme, "none"):
        return lexer_make_token(lexer, TokenType::None, start, lexer.pos)
    
    // Check if it's a keyword
    if is_keyword(lexeme):
        return lexer_make_token(lexer, TokenType::Keyword(lexeme), start, lexer.pos)
    
    return lexer_make_token(lexer, TokenType::Ident(lexeme), start, lexer.pos)

fn lex_string(lexer: Lexer, quote: Int) -> Token:
    let start = lexer.pos
    let _ = lexer_advance(lexer)  // consume opening quote
    let pieces = []
    
    loop:
        if lexer_is_eof(lexer):
            break
        let c = lexer_peek(lexer)
        if c == quote:
            break
        let _ = lexer_advance(lexer)
        
        if c == 92:  // '\\'
            let next = lexer_advance(lexer)
            if next == 110:  // 'n'
                push(pieces, "\n")
            else if next == 116:  // 't'
                push(pieces, "\t")
            else if next == 92:  // '\\'
                push(pieces, "\\")
            else if next == 34:  // '"'
                push(pieces, "\"")
            else if next == 39:  // '\''
                push(pieces, "'")
            else:
                push(pieces, char_from_code(next))
        else:
            push(pieces, char_from_code(c))
    
    let val = join(pieces, "")
    
    if !lexer_is_eof(lexer):
        let _ = lexer_advance(lexer)  // consume closing quote
    
    return lexer_make_token(lexer, TokenType::String(val), start, lexer.pos)

fn lex_multi_char_op(lexer: Lexer, tokens: Array<Token>) -> Bool:
    let c = lexer_peek(lexer)
    let c2 = lexer_peek_n(lexer, 1)
    
    if c == 45 && c2 == 62:  // '->'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::Arrow, "->"))
        return true
    if c == 61 && c2 == 62:  // '=>'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::FatArrow, "=>"))
        return true
    if c == 61 && c2 == 61:  // '=='
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::EqEq, "=="))
        return true
    if c == 33 && c2 == 61:  // '!='
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::NotEq, "!="))
        return true
    if c == 60 && c2 == 61:  // '<='
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::LtEq, "<="))
        return true
    if c == 62 && c2 == 61:  // '>='
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::GtEq, ">="))
        return true
    if c == 38 && c2 == 38:  // '&&'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::And, "&&"))
        return true
    if c == 124 && c2 == 124:  // '||'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::Or, "||"))
        return true
    if c == 58 && c2 == 58:  // '::'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::ColonColon, "::"))
        return true
    if c == 46 && c2 == 46:  // '..'
        let _ = lexer_advance(lexer)
        let _ = lexer_advance(lexer)
        push(tokens, lexer_make_simple_token(lexer, TokenType::DotDot, ".."))
        return true
    
    return false
