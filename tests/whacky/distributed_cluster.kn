# KAIN WHACKY TEST: Distributed Actor Cluster
# Full distributed systems implementation: consensus, replication, partitioning
# Demonstrating actors, effects, async, and pattern matching at scale

println!("=== DISTRIBUTED ACTOR CLUSTER ===")
println!()

# ==========================================
# CLUSTER CONFIGURATION
# ==========================================

struct NodeConfig:
    id: String
    host: String
    port: Int
    role: NodeRole
    datacenter: String

enum NodeRole:
    Leader
    Follower
    Candidate
    Observer

struct ClusterConfig:
    nodes: Array<NodeConfig>
    replication_factor: Int
    read_quorum: Int
    write_quorum: Int
    heartbeat_interval_ms: Int
    election_timeout_ms: Int

# ==========================================
# RAFT CONSENSUS
# ==========================================

struct LogEntry:
    term: Int
    index: Int
    command: Command
    timestamp: Int

enum Command:
    Set(key: String, value: Bytes)
    Delete(key: String)
    CAS(key: String, expected: Bytes, new: Bytes)
    Noop

struct RaftState:
    current_term: Int
    voted_for: Option<String>
    log: Array<LogEntry>
    commit_index: Int
    last_applied: Int
    
    # Leader state
    next_index: Map<String, Int>
    match_index: Map<String, Int>

effect Consensus:
    fn request_vote(candidate_id: String, term: Int, last_log_index: Int, last_log_term: Int) -> VoteResponse
    fn append_entries(leader_id: String, term: Int, entries: Array<LogEntry>, leader_commit: Int) -> AppendResponse
    fn propose(command: Command) -> Result<Int, String>

struct VoteResponse:
    term: Int
    vote_granted: Bool
    voter_id: String

struct AppendResponse:
    term: Int
    success: Bool
    match_index: Int
    node_id: String

actor RaftNode with Consensus:
    var config: NodeConfig
    var state: RaftState
    var role: NodeRole = NodeRole.Follower
    var leader_id: Option<String> = None
    var election_timer: Timer
    var heartbeat_timer: Timer
    var peers: Array<RaftNode>
    
    on Initialize(cfg: NodeConfig, cluster: ClusterConfig):
        config = cfg
        state = RaftState {
            current_term: 0,
            voted_for: None,
            log: [],
            commit_index: 0,
            last_applied: 0,
            next_index: Map.new(),
            match_index: Map.new()
        }
        
        # Start election timer with random jitter
        let timeout = cluster.election_timeout_ms + random_int(0, 150)
        election_timer = Timer.new(timeout)
        send self.ElectionTimeout() after timeout
    
    on ElectionTimeout():
        if role != NodeRole.Leader:
            start_election()
    
    on HeartbeatTimeout():
        if role == NodeRole.Leader:
            send_heartbeats()
            send self.HeartbeatTimeout() after config.heartbeat_interval_ms
    
    fn start_election(self):
        role = NodeRole.Candidate
        state.current_term = state.current_term + 1
        state.voted_for = Some(config.id)
        
        println!(f"[{config.id}] Starting election for term {state.current_term}")
        
        var votes_received = 1  # Vote for self
        let last_log_index = state.log.len() - 1
        let last_log_term = if state.log.is_empty(): 0 else: state.log.last().term
        
        # Request votes from all peers in parallel
        let vote_futures = peers.map(peer => async:
            await peer.request_vote(config.id, state.current_term, last_log_index, last_log_term)
        )
        
        for response in await_all(vote_futures):
            if response.term > state.current_term:
                # Discovered higher term, become follower
                become_follower(response.term)
                return
            
            if response.vote_granted:
                votes_received = votes_received + 1
        
        # Check if we won
        let majority = (peers.len() + 1) / 2 + 1
        if votes_received >= majority:
            become_leader()
    
    fn become_leader(self):
        role = NodeRole.Leader
        leader_id = Some(config.id)
        
        println!(f"[{config.id}] Became LEADER for term {state.current_term}")
        
        # Initialize leader state
        let last_log_index = state.log.len()
        for peer in peers:
            state.next_index.insert(peer.config.id, last_log_index + 1)
            state.match_index.insert(peer.config.id, 0)
        
        # Append a no-op to establish leadership
        let noop = LogEntry {
            term: state.current_term,
            index: state.log.len(),
            command: Command.Noop,
            timestamp: now()
        }
        state.log.push(noop)
        
        # Start sending heartbeats
        send self.HeartbeatTimeout() after 0
    
    fn become_follower(self, term: Int):
        role = NodeRole.Follower
        state.current_term = term
        state.voted_for = None
        
        # Reset election timer
        let timeout = config.election_timeout_ms + random_int(0, 150)
        send self.ElectionTimeout() after timeout
    
    fn send_heartbeats(self):
        for peer in peers:
            let next_idx = state.next_index.get(peer.config.id).unwrap_or(0)
            let entries = state.log[next_idx..]
            
            let prev_log_index = next_idx - 1
            let prev_log_term = if prev_log_index >= 0 and prev_log_index < state.log.len():
                state.log[prev_log_index].term
            else:
                0
            
            spawn async:
                let response = await peer.append_entries(
                    config.id,
                    state.current_term,
                    entries,
                    state.commit_index
                )
                
                if response.success:
                    state.next_index.insert(peer.config.id, response.match_index + 1)
                    state.match_index.insert(peer.config.id, response.match_index)
                    
                    # Check if we can advance commit index
                    update_commit_index()
                else:
                    # Decrement next_index and retry
                    let new_idx = max(0, state.next_index.get(peer.config.id).unwrap() - 1)
                    state.next_index.insert(peer.config.id, new_idx)
    
    fn update_commit_index(self):
        # Find the highest index replicated on a majority
        for n in range(state.commit_index + 1, state.log.len()):
            let replicated_count = 1 + peers.filter(p =>
                state.match_index.get(p.config.id).unwrap_or(0) >= n
            ).len()
            
            let majority = (peers.len() + 1) / 2 + 1
            if replicated_count >= majority and state.log[n].term == state.current_term:
                state.commit_index = n
    
    on request_vote(candidate_id: String, term: Int, last_log_index: Int, last_log_term: Int) -> VoteResponse:
        if term < state.current_term:
            return VoteResponse { term: state.current_term, vote_granted: false, voter_id: config.id }
        
        if term > state.current_term:
            become_follower(term)
        
        # Check if we can vote for this candidate
        let can_vote = match state.voted_for:
            None => true
            Some(id) => id == candidate_id
        
        # Check log is at least as up-to-date
        let our_last_term = if state.log.is_empty(): 0 else: state.log.last().term
        let our_last_index = state.log.len() - 1
        
        let log_ok = last_log_term > our_last_term or 
                     (last_log_term == our_last_term and last_log_index >= our_last_index)
        
        if can_vote and log_ok:
            state.voted_for = Some(candidate_id)
            return VoteResponse { term: state.current_term, vote_granted: true, voter_id: config.id }
        
        return VoteResponse { term: state.current_term, vote_granted: false, voter_id: config.id }
    
    on append_entries(leader_id: String, term: Int, entries: Array<LogEntry>, leader_commit: Int) -> AppendResponse:
        if term < state.current_term:
            return AppendResponse { 
                term: state.current_term, 
                success: false, 
                match_index: 0,
                node_id: config.id 
            }
        
        # Valid leader, reset election timer
        self.leader_id = Some(leader_id)
        become_follower(term)
        
        # Append entries
        for entry in entries:
            if entry.index < state.log.len():
                if state.log[entry.index].term != entry.term:
                    # Conflict - truncate log
                    state.log = state.log[..entry.index]
                    state.log.push(entry)
            else:
                state.log.push(entry)
        
        # Update commit index
        if leader_commit > state.commit_index:
            state.commit_index = min(leader_commit, state.log.len() - 1)
        
        return AppendResponse {
            term: state.current_term,
            success: true,
            match_index: state.log.len() - 1,
            node_id: config.id
        }
    
    on propose(command: Command) -> Result<Int, String>:
        if role != NodeRole.Leader:
            match leader_id:
                Some(id) => return Err(f"Not leader, redirect to {id}")
                None => return Err("No known leader")
        
        let entry = LogEntry {
            term: state.current_term,
            index: state.log.len(),
            command: command,
            timestamp: now()
        }
        state.log.push(entry)
        
        # Wait for replication
        send_heartbeats()
        
        # In production, we'd wait for commit
        return Ok(entry.index)

# ==========================================
# CONSISTENT HASHING FOR PARTITIONING
# ==========================================

struct ConsistentHash:
    ring: BTreeMap<u64, String>
    virtual_nodes: Int

fn consistent_hash_new(nodes: Array<String>, virtual_nodes: Int = 150) -> ConsistentHash:
    var ring = BTreeMap.new()
    
    for node in nodes:
        for i in range(0, virtual_nodes):
            let key = f"{node}:{i}"
            let hash = murmur3_hash(key)
            ring.insert(hash, node)
    
    return ConsistentHash { ring: ring, virtual_nodes: virtual_nodes }

fn get_node(ch: ConsistentHash, key: String) -> String:
    let hash = murmur3_hash(key)
    
    # Find first node >= hash
    match ch.ring.range(hash..).first():
        Some((_, node)) => node
        None => ch.ring.first().unwrap().1  # Wrap around

fn get_replicas(ch: ConsistentHash, key: String, n: Int) -> Array<String>:
    let hash = murmur3_hash(key)
    var replicas: Array<String> = []
    var seen: Set<String> = Set.new()
    
    for (_, node) in ch.ring.range(hash..):
        if not seen.contains(node):
            replicas.push(node)
            seen.insert(node)
            if replicas.len() >= n:
                break
    
    # Wrap around if needed
    if replicas.len() < n:
        for (_, node) in ch.ring.iter():
            if not seen.contains(node):
                replicas.push(node)
                seen.insert(node)
                if replicas.len() >= n:
                    break
    
    return replicas

# ==========================================
# DISTRIBUTED KEY-VALUE STORE
# ==========================================

effect Storage:
    fn get(key: String) -> Option<Bytes>
    fn put(key: String, value: Bytes) -> Result<(), String>
    fn delete(key: String) -> Result<(), String>

actor StorageNode with Storage, Consensus:
    var node_id: String
    var data: Map<String, Bytes>
    var raft: RaftNode
    var hasher: ConsistentHash
    
    on Initialize(id: String, cluster: ClusterConfig):
        node_id = id
        data = Map.new()
        hasher = consistent_hash_new(cluster.nodes.map(n => n.id))
        
        let node_config = cluster.nodes.find(n => n.id == id).unwrap()
        raft = spawn RaftNode()
        send raft.Initialize(node_config, cluster)
    
    on get(key: String) -> Option<Bytes>:
        # Check if this node is responsible
        let responsible = get_replicas(hasher, key, 1)[0]
        if responsible != node_id:
            # Forward to responsible node
            return None  # In production, forward the request
        
        return data.get(key)
    
    on put(key: String, value: Bytes) -> Result<(), String>:
        let replicas = get_replicas(hasher, key, 3)
        
        if not replicas.contains(node_id):
            return Err(f"Not responsible for key, forward to {replicas[0]}")
        
        # Propose to Raft
        let command = Command.Set(key, value)
        let index = raft.propose(command)?
        
        # Wait for commit (simplified)
        data.insert(key, value)
        
        return Ok(())
    
    on delete(key: String) -> Result<(), String>:
        let command = Command.Delete(key)
        raft.propose(command)?
        data.remove(key)
        return Ok(())
    
    on ApplyCommitted(entry: LogEntry):
        match entry.command:
            Set(key, value) => data.insert(key, value)
            Delete(key) => data.remove(key)
            CAS(key, expected, new) =>
                if data.get(key) == Some(expected):
                    data.insert(key, new)
            Noop => ()

# ==========================================
# CLUSTER COORDINATOR
# ==========================================

actor ClusterCoordinator:
    var config: ClusterConfig
    var nodes: Map<String, StorageNode>
    var health: Map<String, NodeHealth>
    
    on Bootstrap(cfg: ClusterConfig):
        config = cfg
        nodes = Map.new()
        health = Map.new()
        
        println!("Bootstrapping cluster with {} nodes...", cfg.nodes.len())
        
        for node_cfg in cfg.nodes:
            let node = spawn StorageNode()
            send node.Initialize(node_cfg.id, cfg)
            nodes.insert(node_cfg.id, node)
            health.insert(node_cfg.id, NodeHealth.Healthy)
        
        # Start health check loop
        send self.HealthCheck() after 1000
    
    on HealthCheck():
        for (id, node) in nodes:
            spawn async:
                match await timeout(node.Ping(), 500):
                    Ok(_) => 
                        if health.get(id) != Some(NodeHealth.Healthy):
                            println!(f"[CLUSTER] Node {id} is now healthy")
                            health.insert(id, NodeHealth.Healthy)
                    Err(_) =>
                        let current = health.get(id).unwrap_or(NodeHealth.Healthy)
                        match current:
                            NodeHealth.Healthy =>
                                println!(f"[CLUSTER] Node {id} missed heartbeat")
                                health.insert(id, NodeHealth.Suspect)
                            NodeHealth.Suspect =>
                                println!(f"[CLUSTER] Node {id} is DOWN")
                                health.insert(id, NodeHealth.Dead)
                                handle_node_failure(id)
                            NodeHealth.Dead => ()
        
        send self.HealthCheck() after 1000
    
    fn handle_node_failure(self, id: String):
        println!(f"[CLUSTER] Initiating failover for node {id}")
        
        # Find partitions owned by this node
        let affected_partitions = get_partitions_for_node(id)
        
        # Reassign to healthy nodes
        let healthy = nodes.keys().filter(k => health.get(k) == Some(NodeHealth.Healthy)).collect()
        
        for partition in affected_partitions:
            let new_owner = healthy[(partition % healthy.len())]
            println!(f"[CLUSTER] Partition {partition} reassigned to {new_owner}")

enum NodeHealth:
    Healthy
    Suspect
    Dead

# ==========================================
# CLIENT INTERFACE
# ==========================================

struct ClusterClient:
    coordinator: ClusterCoordinator
    hasher: ConsistentHash

fn connect(hosts: Array<String>) -> ClusterClient:
    # In production, discover cluster topology
    let coordinator = spawn ClusterCoordinator()
    let hasher = consistent_hash_new(hosts)
    return ClusterClient { coordinator: coordinator, hasher: hasher }

impl ClusterClient:
    fn get(self, key: String) -> Result<Option<Bytes>, String> with Async:
        let node = get_node(self.hasher, key)
        return await self.coordinator.RouteGet(node, key)
    
    fn put(self, key: String, value: Bytes) -> Result<(), String> with Async:
        let replicas = get_replicas(self.hasher, key, 3)
        
        # Write to all replicas
        let futures = replicas.map(node =>
            async: await self.coordinator.RoutePut(node, key, value)
        )
        
        # Wait for quorum
        let results = await_any(futures, count: 2)
        if results.filter(r => r.is_ok()).len() >= 2:
            return Ok(())
        else:
            return Err("Failed to achieve write quorum")

# ==========================================
# MAIN
# ==========================================

fn main() with IO, Async:
    println!("Starting distributed cluster...")
    
    let cluster_config = ClusterConfig {
        nodes: [
            NodeConfig { id: "node-1", host: "127.0.0.1", port: 9001, role: NodeRole.Follower, datacenter: "us-east" },
            NodeConfig { id: "node-2", host: "127.0.0.1", port: 9002, role: NodeRole.Follower, datacenter: "us-east" },
            NodeConfig { id: "node-3", host: "127.0.0.1", port: 9003, role: NodeRole.Follower, datacenter: "us-west" },
            NodeConfig { id: "node-4", host: "127.0.0.1", port: 9004, role: NodeRole.Follower, datacenter: "us-west" },
            NodeConfig { id: "node-5", host: "127.0.0.1", port: 9005, role: NodeRole.Follower, datacenter: "eu-west" },
        ],
        replication_factor: 3,
        read_quorum: 2,
        write_quorum: 2,
        heartbeat_interval_ms: 150,
        election_timeout_ms: 300
    }
    
    let coordinator = spawn ClusterCoordinator()
    send coordinator.Bootstrap(cluster_config)
    
    # Wait for cluster to stabilize
    sleep(2000)
    
    # Test operations
    println!()
    println!("Testing distributed operations...")
    
    let client = connect(["node-1", "node-2", "node-3", "node-4", "node-5"])
    
    # Write some data
    for i in range(0, 100):
        let key = f"key-{i}"
        let value = f"value-{i}".as_bytes()
        client.put(key, value)?
    
    println!("Wrote 100 keys")
    
    # Read back
    var successes = 0
    for i in range(0, 100):
        let key = f"key-{i}"
        match client.get(key)?:
            Some(_) => successes = successes + 1
            None => ()
    
    println!(f"Read back {successes}/100 keys successfully")
    
    println!()
    println!("Distributed cluster is running!")
    println!("Features demonstrated:")
    println!("  - Raft consensus algorithm")
    println!("  - Consistent hashing for partitioning")
    println!("  - Replication with quorum reads/writes")
    println!("  - Failure detection and failover")
    println!("  - Actor-based distributed architecture")
